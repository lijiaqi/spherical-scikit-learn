spsklearn.mixture
=================

.. py:module:: spsklearn.mixture

.. autoapi-nested-parse::

   The :mod:`spsklearn.mixture` module implements mixture modeling algorithms.



Classes
-------

.. autoapisummary::

   spsklearn.mixture.vonMisesFisherMixture


Package Contents
----------------

.. py:class:: vonMisesFisherMixture(n_components=1, *, tol=0.001, max_iter=100, n_init=1, init_params='spherical-k-means++', weights_init=None, means_init=None, kappas_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)

   Bases: :py:obj:`sklearn.mixture._base.BaseMixture`

   .. autoapi-inheritance-diagram:: spsklearn.mixture.vonMisesFisherMixture
      :parts: 1


   von Mises-Fisher Mixture.

   Representation of a von Mises-Fisher mixture model probability distribution.
   This class allows to estimate the parameters of a von Mises-Fisher mixture
   distribution.

   :param n_components: The number of mixture components.
   :type n_components: int, default=1
   :param tol: The convergence threshold. EM iterations will stop when the
               lower bound average gain is below this threshold.
   :type tol: float, default=1e-3
   :param max_iter: The number of EM iterations to perform.
   :type max_iter: int, default=100
   :param n_init: The number of initializations to perform. The best results are kept.
   :type n_init: int, default=1
   :param init_params: The method used to initialize the weights, the means and the
                       precisions.
                       String must be one of:

                       - 'spherical-k-means' : responsibilities are initialized using spherical k-means.
                       - 'spherical-k-means++' : use the spherical k-means++ method to initialize.
                       - 'random' : responsibilities are initialized randomly.
                       - 'random_from_data' : initial means are randomly selected data points.
   :type init_params: {'spherical-k-means', 'spherical-k-means++', 'random', 'random_from_data'},     default='spherical-k-means++'
   :param weights_init: The user-provided initial weights.
                        If it is None, weights are initialized using the `init_params` method.
   :type weights_init: array-like of shape (n_components, ), default=None
   :param means_init: The user-provided initial means,
                      If it is None, means are initialized using the `init_params` method.
   :type means_init: array-like of shape (n_components, n_features), default=None
   :param kappas_init: The user-provided initial concentration parameter kappas.
                       If it is None, kappas are initialized using the 'init_params'
                       method.
   :type kappas_init: array-like, default=None
   :param random_state: Controls the random seed given to the method chosen to initialize the
                        parameters (see `init_params`).
                        In addition, it controls the generation of random samples from the
                        fitted distribution (see the method `sample`).
                        Pass an int for reproducible output across multiple function calls.
                        See :term:`Glossary <random_state>`.
   :type random_state: int, RandomState instance or None, default=None
   :param warm_start: If 'warm_start' is True, the solution of the last fitting is used as
                      initialization for the next call of fit(). This can speed up
                      convergence when fit is called several times on similar problems.
                      In that case, 'n_init' is ignored and only a single initialization
                      occurs upon the first call.
                      See :term:`the Glossary <warm_start>`.
   :type warm_start: bool, default=False
   :param verbose: Enable verbose output. If 1 then it prints the current
                   initialization and each iteration step. If greater than 1 then
                   it prints also the log probability and the time needed
                   for each step.
   :type verbose: int, default=0
   :param verbose_interval: Number of iteration done before the next print.
   :type verbose_interval: int, default=10

   .. attribute:: weights_

      The weights of each mixture components.

      :type: array-like of shape (n_components,)

   .. attribute:: means_

      The mean of each mixture component.

      :type: array-like of shape (n_components, n_features)

   .. attribute:: kappas_

      The concentration parameter kappas of each mixture component.

      :type: array-like

   .. attribute:: converged_

      True when convergence of the best fit of EM was reached, False otherwise.

      :type: bool

   .. attribute:: n_iter_

      Number of step used by the best fit of EM to reach the convergence.

      :type: int

   .. attribute:: lower_bound_

      Lower bound value on the log-likelihood (of the training data with
      respect to the model) of the best fit of EM.

      :type: float

   .. attribute:: n_features_in_

      Number of features seen during :term:`fit`.

      :type: int

   .. attribute:: feature_names_in_

      Names of features seen during :term:`fit`. Defined only when `X`
      has feature names that are all strings.

      :type: ndarray of shape (`n_features_in_`,)

   .. rubric:: Examples

   >>> import numpy as np
   >>> from spsklearn.mixture import vonMisesFisherMixture
   >>> X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])
   >>> vmf = vonMisesFisherMixture(n_components=2, random_state=0).fit(X)
   >>> vmf.means_
   array([[10.,  2.],
          [ 1.,  2.]])
   >>> vmf.predict([[0, 0], [12, 3]])
   array([1, 0])


   .. py:attribute:: weights_init
      :value: None



   .. py:attribute:: means_init
      :value: None



   .. py:attribute:: kappas_init
      :value: None



   .. py:method:: fit_predict(X, y=None)

      Estimate model parameters using X and predict the labels for X.

      The method fits the model n_init times and sets the parameters with
      which the model has the largest likelihood or lower bound. Within each
      trial, the method iterates between E-step and M-step for `max_iter`
      times until the change of likelihood or lower bound is less than
      `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is
      raised. After fitting, it predicts the most probable label for the
      input data points.

      :param X: List of n_features-dimensional data points. Each row
                corresponds to a single data point.
      :type X: array-like of shape (n_samples, n_features)
      :param y: Not used, present for API consistency by convention.
      :type y: Ignored

      :returns: **labels** -- Component labels.
      :rtype: array, shape (n_samples,)



   .. py:method:: bic(X)

      Bayesian information criterion for the current model on the input X.

      You can refer to this :ref:`mathematical section <aic_bic>` for more
      details regarding the formulation of the BIC used.

      :param X: The input samples.
      :type X: array of shape (n_samples, n_dimensions)

      :returns: **bic** -- The lower the better.
      :rtype: float



   .. py:method:: aic(X)

      Akaike information criterion for the current model on the input X.

      You can refer to this :ref:`mathematical section <aic_bic>` for more
      details regarding the formulation of the AIC used.

      :param X: The input samples.
      :type X: array of shape (n_samples, n_dimensions)

      :returns: **aic** -- The lower the better.
      :rtype: float



   .. py:method:: sample(n_samples=1)

      Generate random samples from the fitted von Mises-Fisher mixture distribution.

      :param n_samples: Number of samples to generate.
      :type n_samples: int, default=1

      :returns: * **X** (*array, shape (n_samples, n_features)*) -- Randomly generated sample.
                * **y** (*array, shape (nsamples,)*) -- Component labels.



